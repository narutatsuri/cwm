{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for computing PCS adapted from bootphon/measuring-regularities-in-word-embeddings\n",
    "from util import *\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import auc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = \\\n",
    "    {\"embedding_dir\" : \"../embeddings/\", \n",
    "     \"analogy_dir\" : \"../dataset/BATS_3.0/\", \n",
    "     \"nb_perms\" : 50}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BATS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []; scores = {}\n",
    "\n",
    "if os.path.isdir(args[\"embedding_dir\"]):  \n",
    "    for filename in os.listdir(args[\"embedding_dir\"]):\n",
    "        if \".txt\" in filename:\n",
    "            name = \" \".join(filename.split(\".\")[:-1])\n",
    "            models.append((name, load_model(embedding_dir = os.path.join(args[\"embedding_dir\"], filename))))\n",
    "else:\n",
    "    models.append(load_model(args[\"embedding_dir\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, pairs_sets = bats_names_pairs(dir=args[\"analogy_dir\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute PCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"save_results\"] = \"../results/pcs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc = {}\n",
    "\n",
    "for model in models:\n",
    "    model_sim, model_neg = metrics_from_model(model[1], names, pairs_sets, nb_perms=args[\"nb_perms\"])\n",
    "    model_roc_fpr, model_roc_tpr = compute_roc_curves(model_sim, model_neg, nb_perms=args[\"nb_perms\"])\n",
    "\n",
    "    roc[model[0]] = (model_roc_fpr, model_roc_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_scores = {model[0]:[] for model in models}\n",
    "\n",
    "for index, name in enumerate(names):\n",
    "    fpr_perms = [row[index] for row in column(list(roc.values()), 0)]\n",
    "    tpr_perms = [row[index] for row in column(list(roc.values()), 1)]\n",
    "    x = np.linspace(0, 1, len(min(fpr_perms, key=len)))\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1)\n",
    "\n",
    "    for model in roc:\n",
    "        fpr = np.interp(x, np.linspace(0, 1, len(roc[model][0][index])), roc[model][0][index])\n",
    "        tpr = np.interp(x, np.linspace(0, 1, len(roc[model][1][index])), roc[model][1][index])\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        fig.add_trace(go.Scatter(x=fpr, y=tpr, name=model + f\" (AUC = {auc_score:.4f})\", mode='lines'))\n",
    "        auc_scores[model].append(auc_score)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=name,\n",
    "        xaxis_title='False Positive Rate',\n",
    "        yaxis_title='True Positive Rate',\n",
    "        yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n",
    "        xaxis=dict(constrain='domain'))\n",
    "    fig.write_image(args[\"save_results\"] + name + \".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fpr_means = []; tpr_means = []\n",
    "\n",
    "for model in roc:\n",
    "    fpr, tpr = roc[model]\n",
    "    x = np.linspace(0, 1, len(min(fpr, key=len)))\n",
    "    \n",
    "    fpr = [np.interp(x, np.linspace(0, 1, len(item)), item) for item in fpr]\n",
    "    tpr = [np.interp(x, np.linspace(0, 1, len(item)), item) for item in tpr]\n",
    "\n",
    "    fpr_mean = np.mean(fpr, axis=0)\n",
    "    tpr_mean = np.mean(tpr, axis=0)\n",
    "    fpr_means.append(fpr_mean)\n",
    "    tpr_means.append(tpr_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1)\n",
    "\n",
    "for index, fpr_mean in enumerate(fpr_means):\n",
    "    tpr_mean = tpr_means[index]\n",
    "    fig.add_trace(go.Scatter(x=fpr_mean, y=tpr_mean, name=models[index][0], mode='lines'))\n",
    "    print(\"AUC for {}: {}\".format(models[index][0], np.around(auc(fpr_mean, tpr_mean), 4)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"\",\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n",
    "    xaxis=dict(constrain='domain'),\n",
    "    font={\"size\" : 16})\n",
    "\n",
    "fig.write_image(args[\"save_results\"] + \"total_roc.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for model_name in auc_scores:\n",
    "    fig.add_trace(go.Box(y=auc_scores[model_name], name=model_name))\n",
    "\n",
    "fig.update_layout(title=\"\",\n",
    "                  xaxis_title='', yaxis_title='PCS',\n",
    "                  width=1200, height=1200)\n",
    "\n",
    "fig.write_image(args[\"save_results\"] + \"PCS_box_plot.pdf\")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute MSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"save_results\"] = \"../results/msm/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alignment(vectors):\n",
    "    # Normalize all vectors and compute mean\n",
    "    vectors = [i/np.linalg.norm(i) for i in vectors]\n",
    "    vec_mean = np.mean(vectors, axis=0); vec_mean = vec_mean/np.linalg.norm(vec_mean)\n",
    "\n",
    "    deviations = [np.dot(i, vec_mean) for i in vectors]\n",
    "\n",
    "    return tuple(deviations)\n",
    "\n",
    "def compute_msm(model, names, pairs_sets):\n",
    "    vocab_set = set(list(model.index_to_key))\n",
    "    pairs_sets = [[d for d in list(pairs_sets[i]) if d[0] in vocab_set and d[1] in vocab_set] for i in range(len(pairs_sets))]\n",
    "    name_to_score = {}\n",
    "    for index, pair_set in tqdm(enumerate(pairs_sets), leave=False):\n",
    "        vectors = []\n",
    "        for word_pair in pair_set:\n",
    "            vectors.append(model[word_pair[1]] - model[word_pair[0]])\n",
    "\n",
    "        name_to_score[names[index]] = compute_alignment(vectors)\n",
    "    return name_to_score\n",
    "\n",
    "msm_scores = {}\n",
    "\n",
    "for model in models:\n",
    "    model_score = compute_msm(model[1], names, pairs_sets)\n",
    "    msm_scores[model[0]] = model_score\n",
    "    print(\"DAS for {}: {}\".format(model[0], np.around(np.mean([item for sublist in model_score.values() for item in sublist]), 4)))\n",
    "\n",
    "for name in names:\n",
    "    fig = go.Figure()\n",
    "    for model_name in msm_scores:\n",
    "        fig.add_trace(go.Box(y=msm_scores[model_name][name], name=model_name))\n",
    "\n",
    "    fig.update_layout(title=name, \n",
    "                      xaxis_title='', yaxis_title='Deviation',\n",
    "                      yaxis_range=[0,1],\n",
    "                      width=1200, height=1200)\n",
    "    fig.write_image(args[\"save_results\"] + name + \".pdf\")\n",
    "\n",
    "fig = go.Figure()\n",
    "for model_name in msm_scores:\n",
    "    fig.add_trace(go.Box(y=[item for sublist in msm_scores[model_name].values() for item in sublist], name=model_name))\n",
    "    \n",
    "fig.update_layout(title=\"Total Deviations\",\n",
    "                  xaxis_title='', yaxis_title='Deviation',\n",
    "                  width=1200, height=1200)\n",
    "\n",
    "fig.write_image(args[\"save_results\"] + \"total_deviation.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
